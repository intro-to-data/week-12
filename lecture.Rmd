---
title: "Decision Trees"
output: 
  html_notebook:
    toc: true
    toc_float:
      toc_collapsed: true
    toc_depth: 3
    number_sections: true
    theme: cerule
---



# Regression v Machine learning

- Linear Regression has it's origins in the first decade of the 19th century.
    - Method of Least Squares by Legendre (1805), Gauss (1809)
    - In the early 20th century, this idea was debated and extended by mathematicians such as Yule, Pearson, and Fisher.
    - In the 50's and 60's completing a single regression could take hours.
- Logistic Regression has it's origins in the 1830s and 1840s. 
    - The underlying idea was developed independently several times in different fields of study.
    - Early applications included modeling autocatalytic reactions in chemistry and population growth.
    - Bliss, Gaddum, and Fisher (he is everywhere!) extended these ideas in the 1930s. This work led to the probit model.
        - Probit is a combination of the words probability and unit, prob + it
    - 1940s when Berkson developed the logit function in 1944 which would eventually replace the probit function.
        - Logit or log-odds is the logarithm of the odds p/(1-p) where p is the probability.
        - Remember how I told you there was a relationship between the odds ratio and logistic regression?
        - Well, that's it!
- Although linear and logistic regression are POWERFUL techniques, they hard to apply to increasingly complex data.
    - They are entirely appropriate in cases where the data is constrained in terms of complexity AND suitable domain experts are available.
    - They can mislead when working with large data sets because it so easy to achieve statistical significance, which is often seen as a hallmark of a valid model.



# Setup

```{r include=FALSE}
library(modelr)
library(rio)
library(rpart)
library(rpart.plot)
library(tidyverse)
options(scipen = 999)

titanic <- import("data/titanic-train-clean.csv") |>
  mutate(died = !survived) |>
  select(-survived)
```



# Logistic Regression

- Last week, we did logistic regression.
- Below, I will build the same simple model we did last week.
- But first, we should look at the binomial distribution.
  - Recall that 61.6 of passengers died.
  - Below is a plot of a distribution that is exactly 50/50.
  - There are automated ways to do this in R, but I thought a brute force approach might make sense.

When I toss a coin it can land HEADS or TAILS up. We will toss our coin 15 times per trial. How easy is it to get a count of at least 10 heads out of 15 coin tosses?

Hint: Not very likely.

```{r}
coin <- c("Heads", "Tails")
trials <- 1:10000
trial_counts <- tibble()

for (t in trials) {
  tosses <- sample(coin, 15, replace = TRUE)
  trial_counts <- bind_rows(
    trial_counts,
    tibble(trial = t, n_heads = sum(tosses == "Heads"))
  )
}
```

Our 95% confidence intervals are . . . . (500/10,000)

```{r}
trial_counts |>
  arrange(n_heads) |>
  mutate(as_ordered = row_number()) |>
  filter(as_ordered %in% c(250, 9750))
```

And we can see this in a couple of different ways:

```{r}
trial_counts |>
  count(n_heads) |>
  mutate(p = round(100 * n / sum(n), 1))
```

```{r }
ggplot(trial_counts, aes(x = n_heads)) +
  geom_bar(fill = "lightblue") +
  geom_vline(xintercept = 4, color = "red") +
  geom_vline(xintercept = 11, color = "red")
```

So, to answer our earlier question: 

```{r}
trial_counts |>
  mutate(at_least_10 = if_else(n_heads >= 10, "Yes", "No")) |>
  count(at_least_10) |>
  mutate(p = round(100 * n / sum(n), 1))
```

In a nutshell, the p-value in a logistic regression tells us the percentage chance of the binomial distribution (for our sample size) for the null hypothesis will be more extreme than our sample.

### Model 1

We know that men were more likely to die.

```{r}
titanic |>
  group_by(sex) |>
  summarize(
    n_passengers = n(),
    n_died = sum(died),
    p_died = 100 * n_died / n_passengers
  )
```

In terms of modeling, our null hypothesis would be that men have the same chance as women of dying. Thus, how likely is it that we get an equally or more extreme outcome purely by chance?

This code is a lot like what we did last time.

- did_die is a macabre coin and represents our null hypothesis
- I had to radically increase the number of trials.
  - I recommend AGAINST running that part of the code on Posit Cloud.

```{r}
did_die <- c(rep("Died", 81), rep("Lived", 233))
# In case it is hard to read, this is 100,000.
# I tried to do it 1,000,000 times but gave up.
trials <- 1:100000
titanics <- tibble()

for (t in trials) {
  sank <- sample(did_die, 577, replace = TRUE)
  titanics <- bind_rows(
    titanics,
    tibble(trial = t, how_many_died = sum(sank == "Died"))
  )
}

titanics |>
  mutate(extreme_deaths = if_else(how_many_died > 468, "Yes", "No")) |>
  count(extreme_deaths) |>
  mutate(p = 100 * n / sum(n))
```

So why did we do all that?

1. I want to introduce the concept of sampling the $^*# out of things to replace hard ideas with simple ideas.
2. To show how we can approach statistics DIFFERENTLY than Fisher, etc.
  - Seriously, they would have killed someone to be able to do this.
3. To help you understand what the p-value in this means.

```{r}
glm_sex <- glm(died ~ sex, family = binomial, data = titanic)
summary(glm_sex)
```

- Intercept: `exp(-1.0566)`
- sexmale: `exp(2.5137)` Note the EXTREMELY large number of 0s there.
- Null deviance: How well the response variable can be predicted by a model with only the intercept term.
- Residual deviance: How well the response variable can be predicted by a model with p predictor variables.
  - Lower values tell us the model is better able to predict the response variable.
  - But just chasing this can lead to overfitting.
- AIC: Aikakae Information Criterion
  - AIC uses a model's maximum likelihood estimation (log-likelihood) as a measure of fit.
  - Log-likelihood is a measure of how likely one is to see their observed data, given a model. The model with the maximum likelihood is the one that “fits” the data the best. The natural log of the likelihood is used as a computational convenience.
  - AIC is low for models with high log-likelihoods. This means the model fits the data better, which is what we want. But it adds a penalty term for models with higher parameter complexity, since more parameters means a model is more likely to overfit to the training data.
  - https://builtin.com/data-science/what-is-aic

Now we will build a few more models highlight a limitation of logistic regression for classification.
- But first we need to tweak our data.
- Passenger Class (pclass) is a numeric data type with values 1, 2, and 3.
    - But we should not treat it as a number.
    - There is no class 1.4.
    - And they are not ordinal either, not really.
    - We should treat it as a factor, so we should turn it into a character.

```{r}
## Data skilz to the rescue.
titanic <-
  titanic |>
  mutate(pclass = as.factor(pclass))
```

- Otherwise, logistic regression will try to calculate the odds for every unit increase, which makes no sense.

## Model 2

```{r}
glm_sex_pclass <- glm(died ~ sex + pclass, family = binomial, data = titanic)
summary(glm_sex_pclass)
```

- Note that the Null deviance doesn't change.
- Our residual deviance has dropped from 917.8 to 826.89.
- Our AIC went from 921.8 to 834.89
- That is a big decrease and you can really see why in the pclass 3 estimate.
    - `r exp(1.9055)`
    - For every first class passenger who died, nearly seven third class passengers died. After controlling for differences in gender.

## But, Classification

- When we classify, we can only put people into two buckets, died/survived.
- And we apply all of our criteria at once.
- Watch this.

```{r}
titanic <-
  titanic |>
  add_predictions(
    glm_sex,
    var = "glm_sex",
    type = "response"
  ) |>
  add_predictions(
    glm_sex_pclass,
    var = "glm_sex_pclass",
    type = "response"
  ) |>
  mutate(
    pred_glm_sex = if_else(glm_sex > .5, 1, 0),
    pred_glm_sex_pclass = if_else(glm_sex_pclass > .5, 1, 0)
  )
```

Confusion Matrix: Model 1

```{r}
titanic |>
  group_by(died) |>
  summarize(
    pred_died = sum(if_else(pred_glm_sex == 1, 1, 0)),
    pred_survived = sum(if_else(pred_glm_sex == 0, 1, 0))
  )
```

- Accuracy: `r (468 + 233)/nrow(titanic)`

Confusion Matrix: Model 2

```{r}
titanic |>
  group_by(died) |>
  summarize(
    pred_died = sum(if_else(pred_glm_sex_pclass == 1, 1, 0)),
    pred_survived = sum(if_else(pred_glm_sex_pclass == 0, 1, 0))
  )
```

- Wait.
- Model 2 was BETTER.
- But the confusion matrix is identical.
- Accuracy: `r (468 + 233)/nrow(titanic)`

## So What Happened?

- Our cut-off, risk > .5 was basically arbitrary, although rational.
- And look at the values produced by our models.
- But just look at how our predictions didn't change.
  - `View(titanic)`

# CaRT

- AKA Decision Trees
- The basic concepts of linear and logistic regression were first formulated in the first half of the 19th century.
- And these concepts are still relevant and useful today.
- Today we are going to turn our attention to a machine learning technique called a decision tree. These concepts were first introduced in the 1970s and the formal term Classification and Regression Tree (CaRT) was not coined until 1984.
  - +1 for newer ideas.
- We will use Titanic data to make it easier to see how this technique is similar to and different from logistic regression.

```{r}
## Let's just reset our data.
titanic <- import("data/titanic-train-clean.csv") |>
  select(-child) |>
  mutate(
    ## Note, we are turning pclass into a character again.
    pclass = as.character(pclass),
    died = !survived
  ) |>
  select(-survived)
```

## CaRT Model 1

```{r}
cart_sex <- rpart(died ~ sex, data = titanic, method = "class")
summary(cart_sex)
```

Honestly, nobody reads the summary of decision tree models. . . . because PLOTS!

```{r}
rpart.plot(cart_sex)
```

- I like the graphical nature of decision trees.
- Decision trees focus on purity, not on odds.
- And we can use this model to make predictions.

```{r}
titanic <-
  titanic |>
  add_predictions(cart_sex, var = "pred_cart_sex", type = "class")

titanic |>
  group_by(died) |>
  summarize(
    ## Note the funny matching I have to do.
    ## Sigh. Nothing is ever COMPLETELY consistent.
    pred_died = sum(ifelse(pred_cart_sex == "TRUE", 1, 0)),
    pred_survived = sum(ifelse(pred_cart_sex == "FALSE", 1, 0))
  )
```

- This matches our logistic regression model EXACTLY.
- And that is important, because the results are often SIMILAR.
- Although they work in very different ways.

## CaRT Model 2

```{r}
cart_pclass <- rpart(died ~ pclass, data = titanic, method = "class")
rpart.plot(cart_pclass)
```

- Reading these is a little funky.
- Every step/decision is called a branch.
    - And this is always a boolean test (Yes/No or TRUE/FALSE).
    - Yes always goes LEFT. No always goes RIGHT.
    - GREEN leaves are classified as TRUE.
    - BLUE leaves are classified as FALSE.
    - The percent shown is the percent of the total population.
    - The ratio shown is the ratio of that group which is TRUE.
- Splits are decided on based on Gini Purity.
     - At every point, what split can the computer make in the data to "purify" the data and get as many TRUES on one side?
- The parts at the bottom are called "leaves".
- It is a decision tree!

## CaRT Model 3

```{r}
cart_sex_pclass <- rpart(died ~ sex + pclass, data = titanic, method = "class")
rpart.plot(cart_sex_pclass)
```

- Remember how our confusion matrix didn't change when we added pclass?
- Well, this is telling us that adding pclass to a model with sex doesn't add any more purity to the classification.
- So, although decision trees overfit sometimes, they can also be useful for telling us what not to use.
- I won't do a model here, since the whole point is that it ignored pclass, so it will be just like model 1.


## CaRT Model 4

```{r}
cart_sex_pclass_age <- rpart(died ~ sex + pclass + age, data = titanic, method = "class")
rpart.plot(cart_sex_pclass_age)
```

- Please note how different features enter the model in different places.

Now let's look at a prediction:

```{r}
titanic <-
  titanic |>
  add_predictions(cart_sex_pclass_age, var = "pred_cart_sex_pclass_age", type = "class")

titanic |>
  group_by(died) |>
  summarize(
    ## Note the funny matching I have to do.
    ## Sigh. Nothing is ever COMPLETELY consistent.
    pred_died = sum(if_else(pred_cart_sex_pclass_age == "TRUE", 1, 0)),
    pred_survived = sum(if_else(pred_cart_sex_pclass_age == "FALSE", 1, 0))
  )
```

- Now we can see the decision tree outperforming our logistic regression.
- The number in our bottom left is way up and the number in our bottom right is way down.
- But, we also have to start asking ourselves what part of this is "true" and what part is just a coincidence.

```
# Confusion matrix using the same features via logistic regression.
# A tibble: 2 × 3
  died  predicted_died predicted_survived
  <lgl>          <dbl>              <dbl>
1 FALSE             92                250
2 TRUE             365                184
```

## CaRT Model OMG

- So, let's just throw everything at it.
- But, we want to only use numbers where it makes sense to do so.

```{r}
titanic <- titanic |> select(passengerid:died)
glimpse(titanic)
```

But I think the data types we have left all make "sense".

```{r}
cart_model_omg <- rpart(died ~ ., data = titanic, method = "class")
rpart.plot(cart_model_omg)
```

- So, what happened here?
- You still have to use some common sense.
- Decision trees exploit a pattern. ANY pattern.
- Including "meaningless" things like names and passenger ids.
- Let's try that again.

```{r}
cart_model_omg <- rpart(
  died ~ .,
  ## Subtracts passenger id and name.
  data = titanic |> select(-passengerid, , -name, -ticket, -cabin),
  method = "class"
)
rpart.plot(cart_model_omg)
```

- So we won't use passenger id, name, ticket, and cabin.

```{r}
titanic <-
  titanic |>
  add_predictions(cart_model_omg, var = "pred_cart_model_omg", type = "class")

titanic |>
  group_by(died) |>
  summarize(
    ## Note the funny matching I have to do.
    ## Sigh. Nothing is ever COMPLETELY consistent.
    pred_died = sum(if_else(pred_cart_model_omg == "TRUE", 1, 0)),
    pred_survived = sum(if_else(pred_cart_model_omg == "FALSE", 1, 0))
  )
```

And now we have a model that has an accuracy of: `r (498 + 246)/nrow(titanic)`.

